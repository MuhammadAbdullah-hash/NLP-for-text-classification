{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNwAqxhhtcvCwxKIOOBn4qx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadAbdullah-hash/NLP-for-text-classification/blob/master/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47hJqxotH-Ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "import keras\n",
        "import sklearn.model_selection\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbRkyBZNIiow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ############ Loading and splitting data set ################# #\n",
        "\n",
        "train_data = pd.read_csv('train.csv')\n",
        "\n",
        "tweet = np.array(train_data['text'] , dtype = 'str')\n",
        "target = np.array(train_data['target'])\n",
        "\n",
        "x_train = tweet[0:6850]\n",
        "y_train  = target[0:6850]\n",
        "\n",
        "x_test = tweet[6851:]\n",
        "y_test  = target[6851:]\n",
        "\n",
        "\n",
        "print(x_train.dtype)\n",
        "print(y_train.dtype)\n",
        "\n",
        "print(x_train.shape , y_train.shape)\n",
        "print(x_test.shape , y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3fql5XBK6sP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ############ Tokenizing / Sequencing / Padding ############### #\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 10000 , oov_token= \"<OOV>\")\n",
        "tokenizer.fit_on_texts(x_train) ######### This only generates words data base of train #########\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# ######### We dont tokenize test so that we dont have dat base of test words ############# #\n",
        "\n",
        "sequence_train = tokenizer.texts_to_sequences(x_train)\n",
        "sequence_test = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "pad_train = pad_sequences(sequence_train , padding = 'pre')\n",
        "pad_test = pad_sequences(sequence_test , padding = 'pre')\n",
        "\n",
        "# print(pad_train[0].size)\n",
        "# print(pad_test[0].size)\n",
        "# print(pad_test[0])\n",
        "\n",
        "# print(x_train[0])\n",
        "# print(sequence_train[0])\n",
        "# print()\n",
        "# print(x_test[0])\n",
        "# print(sequence_test[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSfykzpsM9Xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # ############# Model Creation ############### #\n",
        "\n",
        "model = keras.Sequential([\n",
        "keras.layers.Embedding( 10000 , 16 ),  # ######### Creates vectors in diff dimensions ############ #\n",
        "keras.layers.GlobalAveragePooling1D(), ###### Sum up vectors to understand context ########\n",
        "\n",
        "# #### Output and Dense layers #### #\n",
        "\n",
        "keras.layers.Dense(24, activation='relu'),\n",
        "keras.layers.Dense(1, activation='sigmoid'),\n",
        "\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkw-h7nsaqE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()\n",
        "\n",
        "model.fit(pad_train , y_train , epochs=30 , validation_data=(pad_test , y_test))\n",
        "\n",
        "loss,  acc = model.evaluate(pad_test , y_test)\n",
        "\n",
        "print(loss , acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmopCzdnbbKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ######### Checking on validation data set ############ #\n",
        "\n",
        "predictions = model.predict(pad_test)\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  print(  np.round(predictions[i] , 0)   , '\\t' , y_test[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwC45SUub0Zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ########### Making sample submission file ############### #\n",
        "\n",
        "data = pd.read_csv('test.csv')\n",
        "\n",
        "sen = np.array(data['text'])\n",
        "id_num = np.array(data['id'])\n",
        "\n",
        "\n",
        "sequence_sen = tokenizer.texts_to_sequences(sen)\n",
        "pad_sen = pad_sequences(sequence_sen , padding='pre')\n",
        "\n",
        "\n",
        "# print(sen[0])\n",
        "# print(sequence_sen[0])\n",
        "# print(pad_sen[0])\n",
        "\n",
        "# ############## Predictions ############## #\n",
        "\n",
        "prediction = model.predict(pad_sen)\n",
        "prediction = np.round(prediction , 0)\n",
        "final_add = [int(j) for i in prediction for j in i]\n",
        "\n",
        "print(final_add[0])\n",
        "print(type(final_add[0]))\n",
        "z  = { \n",
        "'id' :  id_num ,    \n",
        "'target' : final_add , \n",
        "}\n",
        "\n",
        "sample = pd.DataFrame(z)\n",
        "sample.to_csv('Sample.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}